{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics\n",
    "> Metrics for the evaluation of models are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Union, Optional, Any, Iterable, Callable\n",
    "import os\n",
    "import shutil\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AP(ABC):\n",
    "    \"\"\"Abstarct base class for the AP score and further metrics based on it.\"\"\"\n",
    "    def __init__(self, data, ious=None):\n",
    "        self.data = data\n",
    "        self.ious = ious if ious is not None else np.arange(0.5, 1, 0.05).round(2)\n",
    "    \n",
    "    def get_image_stats(self, gt_boxes, pred_boxes, iou_threshold):\n",
    "        \"\"\"\n",
    "        Returns: tp, fp, fn\n",
    "        \"\"\"\n",
    "        if pred_boxes is None:\n",
    "            return 0,  0, len(gt_boxes)\n",
    "        if len(gt_boxes) == 0:\n",
    "            return 0, len(pred_boxes), 0\n",
    "        else:\n",
    "            # calculate ious and log their mapping with box indices\n",
    "            gt_box_indices = []\n",
    "            pred_box_indices = []\n",
    "            ious = []\n",
    "            for pred_box_index, pred_box in enumerate(pred_boxes):\n",
    "                for gt_box_index, gt_box in enumerate(gt_boxes):\n",
    "                    iou = self.calculate_iou(gt_box, pred_box)\n",
    "                    if iou >= iou_threshold:\n",
    "                        gt_box_indices.append(gt_box_index)\n",
    "                        pred_box_indices.append(pred_box_index)\n",
    "                        ious.append(iou)\n",
    "\n",
    "            # check if any hits happend\n",
    "            if len(ious) == 0:\n",
    "                return 0, len(pred_boxes), len(gt_boxes)\n",
    "            else:\n",
    "                # select matches based on iou\n",
    "                indices_descending = np.argsort(ious)[::-1]\n",
    "                gt_match_indices = []\n",
    "                pred_match_indices = []\n",
    "                for index in indices_descending:\n",
    "                    gt_index = gt_box_indices[index]\n",
    "                    pred_index = pred_box_indices[index]\n",
    "                    if (gt_index not in gt_match_indices) and (pred_index not in pred_match_indices):\n",
    "                        gt_match_indices.append(gt_index)\n",
    "                        pred_match_indices.append(pred_index)\n",
    "                return len(gt_match_indices), len(pred_boxes) - len(pred_match_indices), len(gt_boxes) - len(gt_match_indices)\n",
    "\n",
    "    def get_precision_and_recall(self, gt, pred, iou):\n",
    "        \"\"\"gt and pred need to be sored dicts with the lowest score being the first entry\"\"\"\n",
    "        tps, fps, fns = [], [], []\n",
    "        precisions, recalls, score_thresholds = [], [], []\n",
    "\n",
    "        if pred is None:\n",
    "            return {\n",
    "                \"tp\": np.array([0]), \"fp\": [sum(len(gt_boxes) for gt_boxes in gt.values())], \"fn\": np.array([0]), \n",
    "                \"precision\": np.array([0]), \"recall\": np.array([0]), \"scores\": np.array([0]),\n",
    "                \"ap11\": 0, \"ap\": 0, \"monotonic_recalls\": np.array([0]), \"monotonic_precisions\": np.array([0]),\n",
    "                \"ap11_recalls\": np.array([0]), \"ap11_precisions\": np.array([0])\n",
    "            }\n",
    "\n",
    "        scores = list(pred.keys())\n",
    "        pred_boxes = list(pred.values())\n",
    "        # loop over scores to calculate statistics for the score\n",
    "        for score_index, score in enumerate(scores):\n",
    "            score_tp, score_fp, score_fn = 0, 0, 0\n",
    "            # create dict with active predicitons (prediction with the same or higher score)\n",
    "            active_preds = {}\n",
    "            for pred_entry in pred_boxes[score_index:]:\n",
    "                for filename, bbox in zip(pred_entry[\"filename\"], pred_entry[\"bboxes\"]):\n",
    "                    if filename not in active_preds.keys():\n",
    "                        active_preds[filename] = [bbox]\n",
    "                    else:\n",
    "                        active_preds[filename].append(bbox)\n",
    "            # loop over gt images\n",
    "            for filename, image_gt_boxes in gt.items():\n",
    "                img_tp, img_fp, img_fn = self.get_image_stats(image_gt_boxes, active_preds.get(filename, None), iou)\n",
    "                score_tp += img_tp\n",
    "                score_fp += img_fp\n",
    "                score_fn += img_fn\n",
    "            # calculate precision and recall for the threshold\n",
    "            score_precision = score_tp/(score_tp + score_fp) if score_tp + score_fp > 0 else 0\n",
    "            score_recall = score_tp/(score_tp + score_fn) if score_tp + score_fn > 0 else 0\n",
    "\n",
    "            tps.append(score_tp)\n",
    "            fps.append(score_fp)\n",
    "            fns.append(score_fn)\n",
    "            precisions.append(score_precision)\n",
    "            recalls.append(score_recall)\n",
    "            score_thresholds.append(score)\n",
    "\n",
    "        # convert data to np.arrays for further processing\n",
    "        tps = np.array(tps)\n",
    "        fps = np.array(fps)\n",
    "        fns = np.array(fns)\n",
    "        precisions = np.array(precisions)\n",
    "        recalls = np.array(recalls)\n",
    "        score_thresholds = np.array(score_thresholds)\n",
    "\n",
    "        # calculate additional stats\n",
    "        # AP11\n",
    "        precisions_at_recall_value = []\n",
    "        for recall_value in np.linspace(0.0, 1.0, 11):\n",
    "            indices = np.argwhere(np.array(recalls) >= recall_value).flatten()\n",
    "            precision_max = max(precisions[indices]) if indices.size > 0 else 0\n",
    "            precisions_at_recall_value.append(precision_max)\n",
    "        ap11 = np.mean(precisions_at_recall_value)\n",
    "\n",
    "        #AP\n",
    "        sorted_indices = np.argsort(recalls)\n",
    "        sorted_recalls = recalls[sorted_indices]\n",
    "        sorted_precision = precisions[sorted_indices]\n",
    "        # make the precision values monotonically\n",
    "        calc_recalls = [0] + sorted_recalls.tolist() + [1]\n",
    "        calc_precisions = [0] + sorted_precision.tolist() + [0]\n",
    "        for i in range(len(calc_recalls)-2, -1, -1):\n",
    "            calc_precisions[i] = max(calc_precisions[i], calc_precisions[i+1])\n",
    "        # get indices where the recall value changes\n",
    "        changing_index_list = []\n",
    "        for i in range(1, len(calc_recalls)):\n",
    "            if calc_recalls[i] != calc_recalls[i-1]:\n",
    "                changing_index_list.append(i)\n",
    "        ap = 0.0\n",
    "        for i in changing_index_list:\n",
    "            ap += ((calc_recalls[i]-calc_recalls[i-1])*calc_precisions[i])\n",
    "\n",
    "        return {\n",
    "            \"tp\": tps, \"fp\": fps, \"fn\": fns, \"precision\": precisions, \"recall\": recalls, \"scores\": score_thresholds,\n",
    "            \"ap11\": ap11, \"ap\": ap, \"monotonic_recalls\": np.array(calc_recalls), \"monotonic_precisions\": np.array(calc_precisions),\n",
    "            \"ap11_recalls\": np.linspace(0.0, 1.0, 11), \"ap11_precisions\": np.array(precisions_at_recall_value)\n",
    "        }\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_data(df):\n",
    "        \"\"\"\n",
    "        Needs to prepare the data. The output should be two dict (ground_truths and predictions).\n",
    "        The ground truth dict should have the following structure:\n",
    "            {\n",
    "                class_label:{\n",
    "                    image_name:{\n",
    "                        [Polygon1, Polygon2, ...., PolygonN]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        where Polygon is a shapely polygon.\n",
    "        \n",
    "        The prediciton dict needs to have the following structure:\n",
    "            {\n",
    "                class_label:{\n",
    "                    score:{\n",
    "                        {\n",
    "                            \"polygons\": [Polygon1, Polygon2, ...., PolygonN],\n",
    "                            \"filename\": [filename1, filename2, ...., filenameN]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def filter_data(self, filter_key_word):\n",
    "        \"\"\"Should filter the data for different APs. The keywords will be AP, AP_small, AP_medium, AP_large\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_iou(gt_box, pred_box):\n",
    "        intersection = pred_box.intersection(gt_box)\n",
    "        return intersection.area/(pred_box.area + gt_box.area - intersection.area)\n",
    "        \n",
    "    def get_metric_data(self):\n",
    "        analysis_data = {}\n",
    "        for analysis_type in [\"AP\", \"AP_small\", \"AP_medium\", \"AP_large\"]:\n",
    "            filtered_df = self.filter_data(self.data, analysis_type)\n",
    "            gt_dict, pred_dict = self.prepare_data(filtered_df)\n",
    "            class_names = gt_dict.keys()\n",
    "            class_data = {}\n",
    "            for class_name in class_names:\n",
    "                    iou_data = {}\n",
    "                    for iou in self.ious:\n",
    "                        res = self.get_precision_and_recall(gt_dict[class_name], pred_dict.get(class_name, None), iou)\n",
    "                        iou_data[iou] = res\n",
    "                    iou_data[\"ap\"] = np.array([iou[\"ap\"] for iou in iou_data.values()]).mean()\n",
    "                    class_data[class_name] = iou_data\n",
    "            class_data[\"map\"] = np.array([class_entry[\"ap\"] for class_entry in class_data.values()]).mean()\n",
    "            analysis_data[analysis_type] = class_data\n",
    "        return analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class APObjectDetection(AP):\n",
    "    \"\"\"Implementation of the (m)AP score based metrics for object detection. This implementation uses shapely for calulating the iou. For a faster version see: `APObjectDetectionFast`\"\"\"\n",
    "    @staticmethod\n",
    "    def filter_data(df, filter_key_word):\n",
    "        if filter_key_word == \"AP\":\n",
    "            return df\n",
    "        elif filter_key_word == \"AP_small\":\n",
    "            return df[(df[\"area\"] < 32**2)]\n",
    "        elif filter_key_word == \"AP_medium\":\n",
    "            return df[((32**2 < df[\"area\"]) & (df[\"area\"] < 96**2))]\n",
    "        elif filter_key_word == \"AP_large\":\n",
    "            return df[96**2 < df[\"area\"]]\n",
    "        \n",
    "    @staticmethod\n",
    "    def prepare_data(df):\n",
    "        ground_truth, preds = df[df[\"is_prediction\"] == False].sort_values(\"score\"), df[df[\"is_prediction\"] == True].sort_values(\"score\")\n",
    "\n",
    "        pred_dict = {}\n",
    "        for index, row in preds.iterrows():\n",
    "            polygon = Polygon([[row[\"bbox_xmin\"], row[\"bbox_ymax\"]], [row[\"bbox_xmin\"], row[\"bbox_ymin\"]], [row[\"bbox_xmax\"], row[\"bbox_ymin\"], [row[\"bbox_xmax\"], row[\"bbox_ymax\"]]]])\n",
    "            if row[\"label\"] not in pred_dict.keys():\n",
    "                pred_dict[row[\"label\"]] = {row[\"score\"]: {\"bboxes\": [polygon], \"filename\": [row[\"filename\"]]}}\n",
    "            else:\n",
    "                if not row[\"filename\"] in pred_dict[row[\"label\"]].keys():\n",
    "                    pred_dict[row[\"label\"]][row[\"score\"]] = {\"bboxes\": [polygon], \"filename\": [row[\"filename\"]]}\n",
    "                else:\n",
    "                    pred_dict[row[\"label\"]][row[\"score\"]][\"bboxes\"].append(polygon)\n",
    "                    pred_dict[row[\"label\"]][row[\"score\"]][\"filename\"].append(row[\"filename\"])\n",
    "\n",
    "        gt_dict = {}\n",
    "        for index, row in ground_truth.iterrows():\n",
    "            polygon = Polygon([[row[\"bbox_xmin\"], row[\"bbox_ymax\"]], [row[\"bbox_xmin\"], row[\"bbox_ymin\"]], [row[\"bbox_xmax\"], row[\"bbox_ymin\"], [row[\"bbox_xmax\"], row[\"bbox_ymax\"]]]])\n",
    "            if row[\"label\"] not in gt_dict.keys():\n",
    "                gt_dict[row[\"label\"]] = {row[\"filename\"]: [polygon]}\n",
    "            else:\n",
    "                if not row[\"filename\"] in gt_dict[row[\"label\"]].keys():\n",
    "                    gt_dict[row[\"label\"]][row[\"filename\"]] = [polygon]\n",
    "                else:\n",
    "                    gt_dict[row[\"label\"]][\"filename\"].append(polygon)\n",
    "        return gt_dict, pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class APObjectDetectionFast:\n",
    "    \"\"\"A faster implementaiton for the (m)AP scores.\"\"\"\n",
    "    def __init__(self, data, ious=None):\n",
    "        self.data = data\n",
    "        self.ious = ious if ious is not None else np.arange(0.5, 1, 0.05).round(2)\n",
    "        self.metric_data = self.get_metric_data()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_iou(pred_box, gt_box):\n",
    "        px1, py1, px2, py2 = pred_box\n",
    "        tx1, ty1, tx2, ty2 = gt_box\n",
    "\n",
    "        # return 0 if the boxes don't intersect\n",
    "        if (tx2 < px1 or px2 < tx1 or ty2 < py1 or py2 < ty1):\n",
    "            return 0\n",
    "        else:\n",
    "            lower_x = max(tx1, px1)\n",
    "            upper_x = min(tx2, px2)\n",
    "            lower_y = max(ty1, py1)\n",
    "            upper_y = min(ty2, py2)\n",
    "            intersection_area = (upper_x-lower_x) * (upper_y-lower_y)\n",
    "            gt_box_area = (tx2-tx1) * (ty2-ty1)\n",
    "            pred_box_area = (px2-px1) * (py2-py1)\n",
    "            iou = intersection_area / (gt_box_area + pred_box_area - intersection_area)\n",
    "            return iou\n",
    "    \n",
    "    def get_image_stats(self, gt_boxes, pred_boxes, iou_threshold):\n",
    "        \"\"\"\n",
    "        Returns: tp, fp, fn\n",
    "        \"\"\"\n",
    "        if pred_boxes is None:\n",
    "            return 0,  0, len(gt_boxes)\n",
    "        if len(gt_boxes) == 0:\n",
    "            return 0, len(pred_boxes), 0\n",
    "        else:\n",
    "            # calculate ious and log their mapping with box indices\n",
    "            gt_box_indices = []\n",
    "            pred_box_indices = []\n",
    "            ious = []\n",
    "            for pred_box_index, pred_box in enumerate(pred_boxes):\n",
    "                for gt_box_index, gt_box in enumerate(gt_boxes):\n",
    "                    iou = self.calculate_iou(pred_box, gt_box)\n",
    "                    if iou >= iou_threshold:\n",
    "                        gt_box_indices.append(gt_box_index)\n",
    "                        pred_box_indices.append(pred_box_index)\n",
    "                        ious.append(iou)\n",
    "\n",
    "            # check if any hits happend\n",
    "            if len(ious) == 0:\n",
    "                return 0, len(pred_boxes), len(gt_boxes)\n",
    "            else:\n",
    "                # select matches based on iou\n",
    "                indices_descending = np.argsort(ious)[::-1]\n",
    "                gt_match_indices = []\n",
    "                pred_match_indices = []\n",
    "                for index in indices_descending:\n",
    "                    gt_index = gt_box_indices[index]\n",
    "                    pred_index = pred_box_indices[index]\n",
    "                    if (gt_index not in gt_match_indices) and (pred_index not in pred_match_indices):\n",
    "                        gt_match_indices.append(gt_index)\n",
    "                        pred_match_indices.append(pred_index)\n",
    "                return len(gt_match_indices), len(pred_boxes) - len(pred_match_indices), len(gt_boxes) - len(gt_match_indices)\n",
    "\n",
    "    def get_precision_and_recall(self, gt, pred, iou):\n",
    "        \"\"\"gt and pred need to be sored dicts with the lowest score being the first entry\"\"\"\n",
    "        tps, fps, fns = [], [], []\n",
    "        precisions, recalls, score_thresholds = [], [], []\n",
    "\n",
    "        if pred is None:\n",
    "            return {\n",
    "                \"tp\": np.array([0]), \"fp\": [sum(len(gt_boxes) for gt_boxes in gt.values())], \"fn\": np.array([0]), \n",
    "                \"precision\": np.array([0]), \"recall\": np.array([0]), \"scores\": np.array([0]),\n",
    "                \"ap11\": 0, \"ap\": 0, \"monotonic_recalls\": np.array([0]), \"monotonic_precisions\": np.array([0]),\n",
    "                \"ap11_recalls\": np.array([0]), \"ap11_precisions\": np.array([0])\n",
    "            }\n",
    "\n",
    "        scores = list(pred.keys())\n",
    "        pred_boxes = list(pred.values())\n",
    "        # loop over scores to calculate statistics for the score\n",
    "        for score_index, score in enumerate(scores):\n",
    "            score_tp, score_fp, score_fn = 0, 0, 0\n",
    "            # create dict with active predicitons (prediction with the same or higher score)\n",
    "            active_preds = {}\n",
    "            for pred_entry in pred_boxes[score_index:]:\n",
    "                for filename, bbox in zip(pred_entry[\"filename\"], pred_entry[\"bboxes\"]):\n",
    "                    if filename not in active_preds.keys():\n",
    "                        active_preds[filename] = [bbox]\n",
    "                    else:\n",
    "                        active_preds[filename].append(bbox)\n",
    "            # loop over gt images\n",
    "            for filename, image_gt_boxes in gt.items():\n",
    "                img_tp, img_fp, img_fn = self.get_image_stats(image_gt_boxes, active_preds.get(filename, None), iou)\n",
    "                score_tp += img_tp\n",
    "                score_fp += img_fp\n",
    "                score_fn += img_fn\n",
    "            # calculate precision and recall for the threshold\n",
    "            score_precision = score_tp/(score_tp + score_fp) if score_tp + score_fp > 0 else 0\n",
    "            score_recall = score_tp/(score_tp + score_fn) if score_tp + score_fn > 0 else 0\n",
    "\n",
    "            tps.append(score_tp)\n",
    "            fps.append(score_fp)\n",
    "            fns.append(score_fn)\n",
    "            precisions.append(score_precision)\n",
    "            recalls.append(score_recall)\n",
    "            score_thresholds.append(score)\n",
    "\n",
    "        # convert data to np.arrays for further processing\n",
    "        tps = np.array(tps)\n",
    "        fps = np.array(fps)\n",
    "        fns = np.array(fns)\n",
    "        precisions = np.array(precisions)\n",
    "        recalls = np.array(recalls)\n",
    "        score_thresholds = np.array(score_thresholds)\n",
    "\n",
    "        # calculate additional stats\n",
    "\n",
    "        # AP11\n",
    "        precisions_at_recall_value = []\n",
    "        for recall_value in np.linspace(0.0, 1.0, 11):\n",
    "            indices = np.argwhere(np.array(recalls) >= recall_value).flatten()\n",
    "            precision_max = max(precisions[indices]) if indices.size > 0 else 0\n",
    "            precisions_at_recall_value.append(precision_max)\n",
    "        ap11 = np.mean(precisions_at_recall_value)\n",
    "\n",
    "        #AP\n",
    "        sorted_indices = np.argsort(recalls)\n",
    "        sorted_recalls = recalls[sorted_indices]\n",
    "        sorted_precision = precisions[sorted_indices]\n",
    "        # make the precision values monotonically\n",
    "        calc_recalls = [0] + sorted_recalls.tolist() + [1]\n",
    "        calc_precisions = [0] + sorted_precision.tolist() + [0]\n",
    "        for i in range(len(calc_recalls)-2, -1, -1):\n",
    "            calc_precisions[i] = max(calc_precisions[i], calc_precisions[i+1])\n",
    "        # get indices where the recall value changes\n",
    "        changing_index_list = []\n",
    "        for i in range(1, len(calc_recalls)):\n",
    "            if calc_recalls[i] != calc_recalls[i-1]:\n",
    "                changing_index_list.append(i)\n",
    "        ap = 0.0\n",
    "        for i in changing_index_list:\n",
    "            ap += ((calc_recalls[i]-calc_recalls[i-1])*calc_precisions[i])\n",
    "\n",
    "        return {\n",
    "            \"tp\": tps, \"fp\": fps, \"fn\": fns, \"precision\": precisions, \"recall\": recalls, \"scores\": score_thresholds,\n",
    "            \"ap11\": ap11, \"ap\": ap, \"monotonic_recalls\": np.array(calc_recalls), \"monotonic_precisions\": np.array(calc_precisions),\n",
    "            \"ap11_recalls\": np.linspace(0.0, 1.0, 11), \"ap11_precisions\": np.array(precisions_at_recall_value)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_data(df):\n",
    "        ground_truth, preds = df[df[\"is_prediction\"] == False].sort_values(\"score\"), df[df[\"is_prediction\"] == True].sort_values(\"score\")\n",
    "\n",
    "        pred_dict = {}\n",
    "        for index, row in preds.iterrows():\n",
    "            if row[\"label\"] not in pred_dict.keys():\n",
    "                pred_dict[row[\"label\"]] = {row[\"score\"]: {\"bboxes\": [[row[\"bbox_xmin\"], row[\"bbox_ymin\"], row[\"bbox_xmax\"], row[\"bbox_ymax\"]]], \"filename\": [row[\"filename\"]]}}\n",
    "            else:\n",
    "                if not row[\"filename\"] in pred_dict[row[\"label\"]].keys():\n",
    "                    pred_dict[row[\"label\"]][row[\"score\"]] = {\"bboxes\": [[row[\"bbox_xmin\"], row[\"bbox_ymin\"], row[\"bbox_xmax\"], row[\"bbox_ymax\"]]], \"filename\": [row[\"filename\"]]}\n",
    "                else:\n",
    "                    pred_dict[row[\"label\"]][row[\"score\"]][\"bboxes\"].append([row[\"bbox_xmin\"], row[\"bbox_ymin\"], row[\"bbox_xmax\"], row[\"bbox_ymax\"]])\n",
    "                    pred_dict[row[\"label\"]][row[\"score\"]][\"filename\"].append(row[\"filename\"])\n",
    "\n",
    "        gt_dict = {}\n",
    "        for index, row in ground_truth.iterrows():\n",
    "            if row[\"label\"] not in gt_dict.keys():\n",
    "                gt_dict[row[\"label\"]] = {row[\"filename\"]: [[row[\"bbox_xmin\"], row[\"bbox_ymin\"], row[\"bbox_xmax\"], row[\"bbox_ymax\"]]]}\n",
    "            else:\n",
    "                if not row[\"filename\"] in gt_dict[row[\"label\"]].keys():\n",
    "                    gt_dict[row[\"label\"]][row[\"filename\"]] = [[row[\"bbox_xmin\"], row[\"bbox_ymin\"], row[\"bbox_xmax\"], row[\"bbox_ymax\"]]]\n",
    "                else:\n",
    "                    gt_dict[row[\"label\"]][\"filename\"].append([row[\"bbox_xmin\"], row[\"bbox_ymin\"], row[\"bbox_xmax\"], row[\"bbox_ymax\"]])\n",
    "        return gt_dict, pred_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_data(df, filter_key_word):\n",
    "        if filter_key_word == \"AP\":\n",
    "            return df\n",
    "        elif filter_key_word == \"AP_small\":\n",
    "            return df[(df[\"area\"] < 32**2)]\n",
    "        elif filter_key_word == \"AP_medium\":\n",
    "            return df[((32**2 < df[\"area\"]) & (df[\"area\"] < 96**2))]\n",
    "        elif filter_key_word == \"AP_large\":\n",
    "            return df[96**2 < df[\"area\"]]\n",
    "        \n",
    "    def get_metric_data(self):\n",
    "        analysis_data = {}\n",
    "        for analysis_type in [\"AP\", \"AP_small\", \"AP_medium\", \"AP_large\"]:\n",
    "            filtered_df = self.filter_data(self.data, analysis_type)\n",
    "            gt_dict, pred_dict = self.prepare_data(filtered_df)\n",
    "            class_names = gt_dict.keys()\n",
    "            class_data = {}\n",
    "            for class_name in class_names:\n",
    "                    iou_data = {}\n",
    "                    for iou in self.ious:\n",
    "                        res = self.get_precision_and_recall(gt_dict[class_name], pred_dict.get(class_name, None), iou)\n",
    "                        iou_data[iou] = res\n",
    "                    iou_data[\"ap\"] = np.array([iou[\"ap\"] for iou in iou_data.values()]).mean()\n",
    "                    class_data[class_name] = iou_data\n",
    "            class_data[\"map\"] = np.array([class_entry[\"ap\"] for class_entry in class_data.values()]).mean() if len(class_data.values()) > 0 else 0\n",
    "            analysis_data[analysis_type] = class_data\n",
    "        return analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
